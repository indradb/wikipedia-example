"""
This application will read the TSV generated by crawler.py and create the
vertices/edges in IndraDB. Once completed, the wikipedia dataset will be
explorable.
"""

import os
import sys

import capnp
import indradb
import wikipedia

# Maximum number of items to return per chunk
MAX_CHUNK_SIZE = 50000

class Chunker:
    def __init__(self, filename):
        self.filename = filename
        self.offset = 0
        self.size = os.stat(filename).st_size

    def chunks(self):
        src = None
        buf = []

        with open(self.filename, "r") as f:
            for line in f:
                self.offset += len(line)
                line = line.rstrip()

                if line.startswith("\t"):
                    buf.append((src, line[1:]))
                else:
                    src = line

                if len(buf) >= MAX_CHUNK_SIZE:
                    yield buf
                    buf = []

        if len(buf) > 0:
            yield buf

class Inserter:
    def __init__(self, client):
        self.client = client
        self.article_names_to_ids = {}

    def restore(self, links_chunk):
        """
        Restores the article name to ID mapping, returning whether the
        restoration was successful.
        """

        if len(links_chunk) == 0:
            return True

        # First ensure these links are in IndraDB - checking that the last
        # link exists should suffice
        trans = self.client.transaction()
        edges = trans.get_edges(indradb.SpecificEdgeQuery(indradb.EdgeKey(
            wikipedia.article_uuid(links_chunk[-1][0]),
            "link",
            wikipedia.article_uuid(links_chunk[-1][1]),
        ))).wait()
        if len(edges) == 0:
            return False

        # Set article_names_to_ids values
        for link in links_chunk:
            for article_name in link:
                self.article_names_to_ids[article_name] = wikipedia.article_uuid(article_name)

        return True

    def articles(self, links_chunk):
        """
        From a batch of links, this finds all the unique articles, inserts
        them into IndraDB.
        """
        new_article_names = set()

        # Find all of the unique article names that haven't been inserted before
        for (from_article_name, to_article_name) in links_chunk:
            if from_article_name not in self.article_names_to_ids:
                new_article_names.add(from_article_name)
            if to_article_name not in self.article_names_to_ids:
                new_article_names.add(to_article_name)

        # Create the articles in IndraDB
        items = []

        for article_name in new_article_names:
            vertex_id = wikipedia.article_uuid(article_name)
            items.append(indradb.BulkInsertVertex(indradb.Vertex(vertex_id, "article")))
            items.append(indradb.BulkInsertVertexProperty(vertex_id, "name", article_name))
            self.article_names_to_ids[article_name] = vertex_id

        return self.client.bulk_insert(items)

    def links(self, links_chunk):
        """
        From a batch of links, this inserts all of the links into IndraDB.
        """

        items = []

        for (from_article_name, to_article_name) in links_chunk:
            edge_key = indradb.EdgeKey(
                self.article_names_to_ids[from_article_name],
                "link",
                self.article_names_to_ids[to_article_name],
            )

            items.append(indradb.BulkInsertEdge(edge_key))

        return self.client.bulk_insert(items)

def main():
    restoring = True
    last_promise = capnp.join_promises([]) # Create an empty promise
    chunker = Chunker("data/links.txt")

    with wikipedia.server(bulk_load_optimized=True) as client:
        inserter = Inserter(client)

        for links_chunk in chunker.chunks():
            wikipedia.progress(chunker.offset, chunker.size)

            if restoring:
                if inserter.restore(links_chunk):
                    continue
                restoring = False

            last_promise.wait()
            inserter.articles(links_chunk).wait()
            last_promise = inserter.links(links_chunk)
        
        last_promise.wait()

if __name__ == "__main__":
    main()
