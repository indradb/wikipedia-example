"""
This application will read the TSV generated by crawler.py and create the
vertices/edges in IndraDB. Once completed, the wikipedia dataset will be
explorable.
"""

import os
import sys

import capnp
import wikipedia
import indradb

class Inserter:
    def __init__(self, client):
        self.client = client
        self.article_names_to_ids = {}

    def restore(self, links_chunk):
        """
        Restores the article name to ID mapping, returning whether the
        restoration was successful.
        """

        if len(links_chunk) == 0:
            return True

        # First ensure these links are in IndraDB - checking that the last
        # article has its links should suffice
        last_article_id = wikipedia.article_uuid(links_chunk[-1][0])
        trans = self.client.transaction()
        edge_count = trans.get_edge_count(last_article_id, "link", "outbound").wait()
        
        if edge_count == 0:
            return False

        # Set article_names_to_ids values
        for link in links_chunk:
            for article_name in link:
                self.article_names_to_ids[article_name] = wikipedia.article_uuid(article_name)

        return True

    def articles(self, links_chunk):
        """
        From a batch of links, this finds all the unique articles, inserts
        them into IndraDB.
        """
        new_article_names = set()

        # Find all of the unique article names that haven't been inserted before
        for (from_article_name, to_article_name) in links_chunk:
            if from_article_name not in self.article_names_to_ids:
                new_article_names.add(from_article_name)
            if to_article_name not in self.article_names_to_ids:
                new_article_names.add(to_article_name)

        # Create the articles in IndraDB
        items = []

        for article_name in new_article_names:
            vertex_id = wikipedia.article_uuid(article_name)
            items.append(indradb.BulkInsertVertex(indradb.Vertex(vertex_id, "article")))
            items.append(indradb.BulkInsertVertexProperty(vertex_id, "name", article_name))
            self.article_names_to_ids[article_name] = vertex_id

        return self.client.bulk_insert(items)

    def links(self, links_chunk):
        """
        From a batch of links, this inserts all of the links into IndraDB.
        """

        items = []

        for (from_article_name, to_article_name) in links_chunk:
            edge_key = indradb.EdgeKey(
                self.article_names_to_ids[from_article_name],
                "link",
                self.article_names_to_ids[to_article_name],
            )

            items.append(indradb.BulkInsertEdge(edge_key))

        return self.client.bulk_insert(items)

def main():
    restoring = True
    last_promise = capnp.join_promises([]) # Create an empty promise
    archive_size_mb = os.stat("data/links.tsv").st_size / 1024 / 1024
    offset = 0

    with open("data/links.tsv", "r") as f:
        with wikipedia.server(bulk_load_optimized=True) as client:
            inserter = Inserter(client)

            for lines in wikipedia.grouper(f):
                offset += sum(len(l) for l in lines)
                mb_processed = offset / 1024 / 1024
                wikipedia.progress(mb_processed, archive_size_mb)

                links_chunk = [l.strip().split("\t", maxsplit=1) for l in lines]

                if restoring:
                    if inserter.restore(links_chunk):
                        continue
                    restoring = False

                last_promise.wait()
                inserter.articles(links_chunk).wait()
                last_promise = inserter.links(links_chunk)

            last_promise.wait()

if __name__ == "__main__":
    if len(sys.argv) <= 1:
        raise Exception("No archive path specified")
    else:
        main(sys.argv[1])
